"""Service de gestion des embeddings utilisant SentenceTransformers.

Fournit les fonctionnalit√©s de g√©n√©ration de vecteurs d'embeddings pour la recherche similaire.
"""

import logging
from typing import Optional

import numpy as np
import torch
from fastapi import HTTPException
from sentence_transformers import SentenceTransformer

from app.services.vector_cache import get_cache_instance

logger = logging.getLogger(__name__)

# Constantes
MODEL_NAME = "all-MiniLM-L6-v2"
EXPECTED_DIMENSION = 384
HTTP_500_ERROR = "Erreur interne du service d'embeddings"


class EmbeddingService:
    """Service pour g√©rer les embeddings avec SentenceTransformer."""

    _instance: Optional["EmbeddingService"] = None
    _model: SentenceTransformer | None = None

    def __new__(cls) -> "EmbeddingService":
        """Impl√©mentation du pattern singleton."""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    def get_instance(cls) -> "EmbeddingService":
        """Retourne l'instance singleton du service.

        Returns:
            EmbeddingService: Instance unique du service
        """
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    @property
    def model(self) -> SentenceTransformer:
        """Getter pour le mod√®le d'embedding (lazy loading).

        Returns:
            SentenceTransformer: Instance du mod√®le

        Raises:
            HTTPException: Si le mod√®le ne peut pas √™tre charg√©
        """
        if self._model is None:
            try:
                logger.info("Chargement du mod√®le d'embedding '%s'...", MODEL_NAME)
                self._model = SentenceTransformer(MODEL_NAME)
                logger.info(
                    "Mod√®le charg√© avec succ√®s, dimension=%d",
                    self._model.get_sentence_embedding_dimension(),
                )
            except Exception as e:
                logger.error("Erreur lors du chargement du mod√®le: %s", e)
                raise HTTPException(
                    status_code=500,
                    detail=f"{HTTP_500_ERROR}: impossible de charger le mod√®le",
                ) from e
        return self._model

    @model.setter
    def model(self, model: SentenceTransformer) -> None:
        """Setter pour le mod√®le d'embedding (utilis√© pour les tests).

        Args:
            model: Instance du mod√®le √† utiliser
        """
        self._model = model


def validate_input(text: str) -> None:
    """Valide le texte d'entr√©e.

    Args:
        text: Texte √† valider

    Raises:
        ValueError: Si le texte est invalide
    """
    if text is None:
        raise ValueError("Le texte ne peut pas √™tre None")
    if not text.strip():
        raise ValueError("Le texte ne peut pas √™tre vide")


def normalize_vector(vector: np.ndarray) -> np.ndarray:
    """Normalise un vecteur d'embedding.

    Args:
        vector: Vecteur √† normaliser

    Returns:
        np.ndarray: Vecteur normalis√©
    """
    norm = np.linalg.norm(vector)
    if norm > 0:
        vector = vector / norm
    return vector


def embed_text(text: str) -> list[float]:
    """G√©n√®re un embedding pour un texte donn√©.

    Args:
        text: Texte √† transformer en embedding

    Returns:
        list[float]: Vecteur d'embedding normalis√©

    Raises:
        HTTPException: En cas d'erreur lors de la g√©n√©ration
        ValueError: Si le texte est invalide
    """
    try:
        validate_input(text)

        # V√©rifier si l'embedding est d√©j√† dans le cache
        cache = get_cache_instance()
        cached_embedding = cache.get_embedding(text)

        if cached_embedding is not None:
            logger.debug("Embedding trouv√© dans le cache")
            return cached_embedding.tolist()

        # Si non trouv√© en cache, calculer l'embedding
        model = EmbeddingService.get_instance().model
        with torch.no_grad():
            embedding = model.encode(
                text, convert_to_tensor=True, normalize_embeddings=True
            )
        embedding = embedding.cpu().numpy()
        embedding = normalize_vector(embedding)

        # V√©rifier la dimension selon que c'est un vecteur 1D ou 2D
        if embedding.ndim == 1:
            if embedding.shape[0] != EXPECTED_DIMENSION:
                raise ValueError(
                    f"Dimension incorrecte: {embedding.shape[0]}, "
                    f"attendu: {EXPECTED_DIMENSION}"
                )
        elif embedding.shape[1] != EXPECTED_DIMENSION:
            raise ValueError(
                f"Dimension incorrecte: {embedding.shape[1]}, "
                f"attendu: {EXPECTED_DIMENSION}"
            )

        # Stocker l'embedding dans le cache
        cache.store_embedding(text, embedding)

        logger.info(
            "Embedding g√©n√©r√© avec succ√®s: min=%f, max=%f, norme=%f",
            np.min(embedding),
            np.max(embedding),
            np.linalg.norm(embedding),
        )

        return embedding.tolist()
    except ValueError as ve:
        logger.error("Erreur de validation: %s", ve)
        raise HTTPException(status_code=400, detail=str(ve)) from ve
    except Exception as e:
        logger.error("Erreur lors de la g√©n√©ration de l'embedding: %s", e)
        raise HTTPException(
            status_code=500, detail=f"{HTTP_500_ERROR}: {str(e)}"
        ) from e


def generate_query_vector(query: str) -> np.ndarray:
    """G√©n√®re un vecteur de requ√™te pour la recherche.

    Args:
        query: Texte de la requ√™te

    Returns:
        np.ndarray: Vecteur de requ√™te normalis√© et correctement dimensionn√© (2D)

    Raises:
        ValueError: Si la requ√™te est invalide
        HTTPException: En cas d'erreur de g√©n√©ration
    """
    try:
        validate_input(query)
        logger.info("G√©n√©ration du vecteur de requ√™te pour: '%s'", query[:50] + "..." if len(query) > 50 else query)

        # V√©rifier si le vecteur est d√©j√† dans le cache
        cache = get_cache_instance()
        cached_vector = cache.get_embedding(query)

        if cached_vector is not None:
            logger.info("‚úÖ Vecteur de requ√™te trouv√© dans le cache")

            # S'assurer que le format est correct (2D)
            if cached_vector.ndim == 1:
                cached_vector = cached_vector.reshape(1, -1)

            # Log des statistiques du vecteur
            norm = np.linalg.norm(cached_vector)
            mean = np.mean(cached_vector)
            std = np.std(cached_vector)
            logger.info(
                "üìä Statistiques du vecteur (cache): norme=%.4f, moyenne=%.4f, √©cart-type=%.4f",
                norm, mean, std
            )
            return cached_vector

        # Si non trouv√© en cache, calculer le vecteur
        model = EmbeddingService.get_instance().model
        with torch.no_grad():
            # Force la g√©n√©ration d'embeddings en mode batch (1 √©l√©ment) pour assurer un tenseur 2D
            vector = (
                model.encode([query], convert_to_tensor=True, normalize_embeddings=True)
                .cpu()
                .numpy()
            )

        # V√©rification de la dimension attendue
        if vector.shape[1] != EXPECTED_DIMENSION:
            raise ValueError(
                f"Dimension incorrecte: {vector.shape[1]}, "
                f"attendu: {EXPECTED_DIMENSION}"
            )

        vector = normalize_vector(vector)

        # Stocker le vecteur dans le cache
        cache.store_embedding(query, vector)

        # Log d√©taill√© sur le vecteur g√©n√©r√©
        norm = np.linalg.norm(vector)
        mean = np.mean(vector)
        std = np.std(vector)
        min_val = np.min(vector)
        max_val = np.max(vector)
        non_zeros = np.count_nonzero(vector)
        logger.info(
            "üìä Statistiques du vecteur (g√©n√©r√©): dimension=%s, norme=%.4f, moyenne=%.4f, √©cart-type=%.4f",
            vector.shape, norm, mean, std
        )
        logger.info(
            "üìà D√©tails suppl√©mentaires: min=%.4f, max=%.4f, valeurs non-nulles=%d/%d",
            min_val, max_val, non_zeros, vector.size
        )

        return vector
    except ValueError as ve:
        logger.error("‚ùå Erreur de validation: %s", ve)
        raise
    except Exception as e:
        logger.error("‚ùå Erreur lors de la g√©n√©ration du vecteur: %s", e)
        raise HTTPException(
            status_code=500, detail=f"{HTTP_500_ERROR}: {str(e)}"
        ) from e
